% !root= Exam 4.tex
\documentclass{article}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{setspace}
\usepackage{units}
\usepackage{graphicx}
\usepackage{amsopn}
\usepackage{bbding}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{etoolbox}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{numerica}
\usepackage[top=0.1in, left=0.1in, right=0.1in, bottom=0.1in]{geometry}
\AtBeginEnvironment{document}{\everymath{\displaystyle}}
\pagestyle{plain}
\title{EXAM 4}
\author{MATH 1B, Bach}
\date{Tejas Patel, Roll \#30}
\begin{document}

\begin{multicols*}{2}
\footnotesize Row Operations: \\ Swap, Addition, Multiplication
\\RREF: pivot rows have leading value of 1, RREF is unique
\\Test 1:$a=\begin{bmatrix}2\\-1\\2\end{bmatrix} \begin{bmatrix}-2\\3\\1\end{bmatrix} b= \begin{bmatrix}4\\0\\7\end{bmatrix}$
\\Can $b$ be written as a linear combination of $a_1$ and $a_2$? \\ Ax=b $\begin{bmatrix}2&-2&4\\-1&3&0\\2&1&7\end{bmatrix}$ 
RREF $\begin{bmatrix}1&0&3\\0&1&1\\0&0&0\end{bmatrix}\, c_1=3,c_2=1 $ for $c_1a_1+c_2a+2=b$\\Prove a set of 3 vectors in $\mathbb{R}^2$ always spans $\mathbb{R}^2$. C/E: $\begin{bmatrix}0\\1\end{bmatrix}\begin{bmatrix}0\\1\end{bmatrix}\begin{bmatrix}0\\1\end{bmatrix}$
\\\textbf{Ways to prove linear dependence}. Find relation, multiples, trivial solution, or more vectors than variables
\\Traffic flow network eqs in=out
$600=x_1+x_4,x_3+x_4=300,\\x_2+x_1=x_5+700, x_2+x_3=700$ put it in matrix \& RREF. General Solution is all variables in terms of const and free vars. If ask for contstraints, make car number not go negative yk.
\\Let $T$ transform $u \begin{bmatrix}-2\\-3\end{bmatrix}$to$ \begin{bmatrix}-3\\4\end{bmatrix}$ and $v \begin{bmatrix}5\\-1\end{bmatrix}$to$ \begin{bmatrix}16\\10\end{bmatrix}$ Find $3u+2v$
\\=$3\begin{bmatrix}-3\\4\end{bmatrix}+2\begin{bmatrix}16\\10\end{bmatrix}=\begin{bmatrix}23\\-8\end{bmatrix}$
\\$A=\begin{bmatrix}1&-2&-1\\0&5&0\\3&-6&-3\end{bmatrix}b=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}$ determine if Ax=b is consistent for all $b_1,b_2,b_3$.
\\$\begin{bmatrix}1&-2&-1&b_1\\0&5&0&b_2\\3&-6&-3&b_3\end{bmatrix}$ RREF to $b_1=x_1-2x_2-x_3, b_2=5x_2, b_3=3b_1$ \textbf{Not consistent for all b/c its possible last row = 0,0,0,$\not$0}
\\Explain why this means the transformation isnt onto: $b_3-3b_1$ will always be 0, meaning another value of $b_3-3b_1$ is not mapped meaning its not onto, counterexample (2,0,6).
\\Solve the matrix equation for X. A,B,X are square. A-B is invertible. Check work. \\ $AX-BX=A \rightarrow X(A-B)=A \rightarrow X=(A-B)^{-1}A$ prove using A=1,2,3,4 and B=2,3,4,5
\\Let $A=\begin{bmatrix}1&1&1\\0&p-2&-1\\0&0&3\end{bmatrix},b=\begin{bmatrix}-2\\h\\15\end{bmatrix}$ Determine all p and h so the system is inconsistent. If $p=2$ and $h\neq -5$. Unique solution: All h's except -5 and p=2. Infinite solutions when p=2,h=-5 since $x_2$ will be a free variable.
\\Transformation $e_1 \rightarrow 4e_1+e_2, e_2$ is reflected across the x axis. (1,0) $\rightarrow$ (4,1) and (0,1)$\rightarrow$(0,-1) so the transformation matrix is (4,0 and 1,-1). Find T(3,5). plug into transformation matrix and get (12,-2)
\\Find $A^{-1}$ and write $A$ as a product of elementary matrices \\$\begin{bmatrix}1&-2\\-3&5\end{bmatrix}$ Augment with elementary matrix to get $\begin{bmatrix}-5&-2\\-3&-1\end{bmatrix}$
\\Two write as a product of elementary, Row reduce, reverse the operation, apply it to elementary matrix. For this problem: 
\\Row Reduce, $R_2 += 3R_1, R_2*=-1, R_1+=R_2 \Rightarrow R_2-=3R_1, R_2/=-1, R_1-=2R_2$ Then apply it to elementary matrices, index, and write it out
$e_1=\begin{bmatrix}1&0\\-3&1\end{bmatrix},e_2=\begin{bmatrix}1&0\\0&-1\end{bmatrix},e_3=\begin{bmatrix}1&-2\\0&1\end{bmatrix}$ then write it out $A=e_3^{-1}e_2^{-1}e_1^{-1}I$ dont forget identity matrix at the end to make it work.
\\Determine the standard matrix for $\begin{bmatrix}-4x_2-4x_3\\3x_1+7x_2+13x_3\\-x_1-2x_3\\4x_2+4x_3\end{bmatrix}$its $\begin{bmatrix}0&-4&-4\\3&7&13\\-1&0&-2\\0&4&4\end{bmatrix}$ in parametric vector form $x_1\begin{bmatrix}0\\3\\-1\\0\end{bmatrix}+x_2\begin{bmatrix}-4\\7\\0\\4\end{bmatrix}+x_3\begin{bmatrix}-4\\13\\-2\\4\end{bmatrix}=\begin{bmatrix}-8\\8\\2\\8\end{bmatrix}$\\

Practice Test 2:\\
\columnbreak

Test 2: \\
$A=\begin{bmatrix}1&2&0&0&1&6&4\\-2&-4&0&0&-2&-12&-8\\1&2&1&2&1&6&5 \\0&0&2&4&-1&-2&-6\end{bmatrix}=\begin{bmatrix}1&2&0&0&0&4&-4\\0&0&1&2&0&0&1\\0&0&0&0&1&2&8\\0&0&0&0&0&0&0\end{bmatrix}$
Rank = 3, Nullity = 4, Rank + Nullity = Columns. Nul A $\in \mathbb{R}^3$ False, Row A = Row(RREF(A)) True, Col(A)=Col(RREF(A)) True, Col($A^T$)=Row(A) True.
\\Basis for Row(A) $[1,2,0,0,0,4,-4],[0,0,1,2,0,0,1],[0,0,0,0,1,2,8]$ \\ Basis for Col A $\begin{bmatrix}1\\-2\\1\\0\end{bmatrix}, \begin{bmatrix}0\\0\\1\\2\end{bmatrix}\begin{bmatrix}1\\-2\\1\\-1\end{bmatrix}$ 
 Nul A: Take each row of RREF(A), set it equal to 0,
 Dependence relation: you know how to find it but its $2x_1=x_2$
 $
\textbf{Cofactor Expansion (Laplace Expansion)} \\
\text{Choose any row }k\text{ or any column }k\text{ (pick one with the most zeros).}\\[6pt]
\ \underline{\text{Example }(3\times3):}\quad\det\begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i\end{pmatrix}= a\begin{vmatrix} e & f\\ h & i \end{vmatrix}- b\begin{vmatrix} d & f\\ g & i \end{vmatrix}+ c\begin{vmatrix} d & e\\ g & h \end{vmatrix}.$
\\If A is 3x3 and $A^2=3A$ finall values of $\det$ A. A is either 3I or 0, so 27 or 0
\\If Gx=y has a solution for every y in $\mathbb{R}^n$ will the columns of G be LI? Why or why not? No G may be overdetermined where some olumns may be redundant. To remove redundancy, G must have at most n columns.
\\Suppose A is an nxn matrix with eigenvalue $\lambda$ and eigenvector v. If 3v and eigenvector of A? If so, what is the correspopnding eigenvalue? Yes, it is bc the vector itself doesnt change its just being scaled. The vector is still tied to whatever the eigenvalue is and it wont change.
\\If V=$\mathbb{R}^2$ and B=$[b_1,b_2]=\begin{bmatrix}1&-2\\-3&5\end{bmatrix}C=[c_1,c_2]=\begin{bmatrix}1&2\\1&1\end{bmatrix}$ are bases. Find the change of coordinate matrix P from C to B. Show that row reductions of $[B|C]==[I|P]$ gives P. Result should equal $\begin{bmatrix}-7&-12\\-4&-7\end{bmatrix}$
\\For x=[1,2] find the coordinates for x in both the basis B and C. Augment the basis matrices one by one with the given coordinate and row reduce.
\\Let A be an mxn matrix. Suppose the nullspace of A is a plane in $\mathbb{R}^3$ and the range is spanned by a nonzero vector in $\mathbb{R}^5$ so Col(A)=Span(v). Determine m and n. Also find rank and nullity of A.Let $A$ be an $m \times n$ matrix.\\
Given:$\text{Nul}(A)$ is a plane in $\mathbb{R}^3$ $\Rightarrow$ nullity $= 2$, $n = 3$
- $\text{Col}(A) = \text{Span}(\mathbf{v}) \subset \mathbb{R}^5$ $\Rightarrow$ rank $= 1$, $m = 5$
\\By the Rank-Nullity Theorem:$\text{rank} + \text{nullity} = n \Rightarrow 1 + 2 = 3$
\\Answer: $m = 5$, $n = 3$, rank $= 1$, nullity $= 2$
\\Let $1-t+6t^2, 5-3t, t-15t^2$. Use coordinate vectors to show theyre linearly dependent and give a relation. How to: Turn them into vectors, put them into a matrix, row reduce until you get a row of zeroes, thats proof. Then match degrees on opposite sides of an equal sign to get a dependence relation. Degree matching works since its linear.
\\Let $p_1$ augmented with $p_2$ be a basis for h, find coordinates of $1-15t^2$ and $6-4t+6t^2$in h. Just use degree matching since its linear. Answer is -2.5,0,5 \& 1,1
\\Explain why $1-15t^2$ and $6-4t+6t^2$ is a basis for h: The coordinate matrix [-2,5,1,0,5,1] row reduces to the identity matrix, meaning the vectors are LI and since thats true they also form a basis for H
\\\textbf{Test 3:} 
\\Find the Steady state probability vector: $\begin{bmatrix}0.3&0.1\\0.7&0.9\end{bmatrix}$. Take the matrix to the infinite power. Other option is $\begin{bmatrix}0.3&0.1\\0.7&0.9\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}x_1\\x_2\end{bmatrix}$ and solve system.
\\Let u=$\begin{bmatrix}1\\2\\3\end{bmatrix}$, and let Q be the set of vectors x in $\mathbb{R}^3$ for which $u \cdot x = 0$
\\Show $W$ is a subspace of $\mathbb{R}^3$\\
Zero vector: $u\cdot 0=0$, so $0\in W$.
\\Closed under addition: If $x,y\in W$, then $u\cdot(x+y)=u\cdot x+u\cdot y=0+0=0\Rightarrow x+y\in W$.
\\Closed under scalar multiplication: For any scalar $c$ and $x\in W$, $u\cdot(cx)=c(u\cdot x)=c\cdot0=0\Rightarrow cx\in W$.
\\Geometric description: $u$ is a normal vector to $W$, so $W$ is the plane through the origin that is orthogonal (perpendicular) to $u$.
\\Basis for $W$
\\Write a generic vector $x=(x_1,x_2,x_3)$ and impose $u\cdot x=0$
\\$1\cdot x_1 + 2\cdot x_2 + 3\cdot x_3 = 0 \quad\Longrightarrow\quad x_1 = -2x_2 - 3x_3$
\\Choose free parameters $s=x_2$ and $t=x_3$
\\$x=(-2s-3t,\;s,\;t)=s\begin{bmatrix}-2\\1\\0\end{bmatrix}+t\begin{bmatrix}-3\\0\\1\end{bmatrix}$
\\Thus a convenient basis is $\begin{bmatrix}-2\\1\\0\end{bmatrix} \begin{bmatrix}-3\\0\\1\end{bmatrix}$
\end{multicols*}
\pagebreak
\begin{multicols*}{2}
\footnotesize
Let $W$ be the subspace spanned by the $v$'s. Write $y$ as the sum of a vector, $\hat{y}$, in $W$ and a vector, $z$, orthogonal to $W.$
$\quad y = \begin{bmatrix} 24 \\ 3 \\ 6 \end{bmatrix}, \quad v_1 = \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}, \quad v_2 = \begin{bmatrix} 2 \\ 1 \\ 2 \end{bmatrix}$
Projection of $y$ onto $W$
\\Solve $y=c_1v_1+c_2v_2+z$ with $z\perp W$ and set up $(A^\top A)\mathbf{c}=A^\top y$, where $A=[v_1;v_2]$
$A^\top A=\begin{bmatrix}2 & -2\\-2 & 9\end{bmatrix},\, A^\top y=\begin{bmatrix}15\\101\end{bmatrix} $Solution: $\mathbf{c}=\begin{bmatrix}9\\7\end{bmatrix}$, i.e. $c_1=9,;c_2=7$
\\Hence $\hat y=c_1v_1+c_2v_2=9\!\begin{bmatrix}1\\0\\-1\end{bmatrix}+7\!\begin{bmatrix}2\\1\\2\end{bmatrix}=\begin{bmatrix}23\\7\\5\end{bmatrix}$. Orthogonal component$z=y-\hat y=\begin{bmatrix}24\\3\\6\end{bmatrix}-\begin{bmatrix}23\\7\\5\end{bmatrix}=\begin{bmatrix}1\\-4\\1\end{bmatrix}.$Check: $v_1!\cdot! z=0,; v_2!\cdot! z=0$, so $z\perp W$.$\boxed{\;y=\hat y+z=\begin{bmatrix}23\\7\\5\end{bmatrix}+\begin{bmatrix}1\\-4\\1\end{bmatrix}\;}$Thus $\hat y\in W$ and $z$ is the required orthogonal complement.
\\Orthonormal basis containing $v_1,;v_2,;z \quad z=\begin{bmatrix}1\\-4\\1\end{bmatrix}$ (the component of $y$ perpendicular to $W$)
\\Norms: $\lVert v_1\rVert=\sqrt{1^{2}+0^{2}+(-1)^{2}}=\sqrt{2},\quad\lVert v_2\rVert=\sqrt{2^{2}+1^{2}+2^{2}}=3,\quad\lVert z\rVert=\sqrt{1^{2}+(-4)^{2}+1^{2}}=3\sqrt{2}.$
\\Unit (orthonormal) vectors: $e_1=\frac{1}{\sqrt{2}}\begin{bmatrix}1\\0\\-1\end{bmatrix},\qquad e_2=\frac{1}{3}\begin{bmatrix}2\\1\\2\end{bmatrix},\qquad e_3=\frac{1}{3\sqrt{2}}\begin{bmatrix}1\\-4\\1\end{bmatrix}.$
\\Coordinates of $x$ in that basis: Given $x=\begin{bmatrix}-14\\-5\\-5\end{bmatrix}$, its expansion in the orthonormal basis ${e_1,e_2,e_3}$ is obtained by plain dot-products
\\$\begin{aligned}c_1 &= x\cdot e_1=\frac{-14(1)+0(-5)+(-1)(-5)}{\sqrt{2}}=\frac{-14+5}{\sqrt{2}}=-\frac{9}{\sqrt{2}},\\[6pt]c_2 &= x\cdot e_2=\frac{-14(2)+(-5)(1)+(-5)(2)}{3}=\frac{-28-5-10}{3}=-\frac{43}{3},\\[6pt]c_3 &= x\cdot e_3=\frac{-14(1)+(-5)(-4)+(-5)(1)}{3\sqrt{2}}=\frac{-14+20-5}{3\sqrt{2}}=\frac{1}{3\sqrt{2}}.\end{aligned}$
\\Let $A$ be $m\times n$ matrix. Nul $A$ is a  line in $R^3$ and the range is spanned by two nonzero vectors $v_1,v_2$ in $\mathbb{R}^5$ so Col $A=$ Span$ (v_1,v_2)$. Determine m and n, also fina rank and nullity
\\$\text{Nul},A$ sits inside the domain $\mathbb{R}^n$. You’re told it’s a line in $\mathbb{R}^3$, so $n=3$ nullity=1. $\text{Col},A$ lives in the codomain $\mathbb{R}^m$ It’s spanned by two independent vectors $v_1,v_2\in\mathbb{R}^5$, so m=5 rank=2. Rank-nullity check: $\text{rank}+\text{nullity}=2+1=3=n$. Answer: $m=5,; n=3,; \text{rank}(A)=2,; \text{nullity}(A)=1.$
\\Define the map $T : M_{2 \times 2} \to P_1$ by $T\left( \begin{bmatrix} a & b \\ c & d \end{bmatrix} \right) = (a + 9c + 5d) t + (c - 4d) t$
\\Example $T(f(t)) = \begin{bmatrix} 1 & 4 \\ 3 & 2 \end{bmatrix} = 38 + (-5)t$ Find $T\left( \begin{bmatrix} -1 & 7 \\ 1 & 2 \end{bmatrix} \right) = 2-9t$
\\$T\left( \begin{bmatrix} -41 & 0 \\ 4 & 1 \end{bmatrix} \right) = 0$ its special bc it maps to the 0 polynomial
\\Assume T is a linear transformation. Find the matrix A for T relative to $C = \left\{ \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \right\}$and $B = \{ 1, t \}$, the standard bases for $M_{2 \times 2}$ and $P_1$, respectively.
$C=!{E_{11},E_{12},E_{21},E_{22}}$ and $B={1,t}$ \\ $A_{B\leftarrow C}=\begin{bmatrix}T(E_{11}) & T(E_{12}) & T(E_{21}) & T(E_{22})\end{bmatrix}_{B}=\begin{bmatrix}1 & 0 & 9 & 5\\0 & 0 & 1 & -4\end{bmatrix}.$
\\Column space: $\operatorname{Col}(A)=\operatorname{span}{\begin{bmatrix}1\\0\end{bmatrix},\begin{bmatrix}9\\1\end{bmatrix}}.$Rank $=2$
\\Null space (solve $Ax=0$): $\operatorname{Nul}(A)=\operatorname{span}{\begin{bmatrix}0\\1\\0\\0\end{bmatrix},\begin{bmatrix}-41\\0\\4\\1\end{bmatrix}}$ and nullity $=2$
\\Range and kernel of $T$ Range: The two independent images $1$ and $9+t$ already span $P_1$, so $\displaystyle\operatorname{Range}(T)=P_1.$
\\Kernel (translate the basis vectors of $,\operatorname{Nul}(A)$ back to matrices) $\ker T=\operatorname{span}\!\{\underbrace{\begin{bmatrix}0&1\\0&0\end{bmatrix}}_{E{12}},\;\underbrace{\begin{bmatrix}-41&0\\4&1\end{bmatrix}}_{X_2}\}.$
\vfill\columnbreak
Let $P_1$ be the vector space of all real polynomials of degree 1 or less. Consider the linear transformation
$T : P_1 \rightarrow P_1$ defined by $T(a + bt) = (4a + b) + (-b)t$ for any $a + bt \in P_1$. Example, $T(2 + t) = 9 - t$
\\Image of $f(t)=3-10t$: $T(3-10t)=(4\cdot 3 + (-10)) + (-(-10))t = 2 + 10t.$
\\Matrix of $T$ in the standard basis $E={1,t}$: $T(a+bt)=(4a+b) + (-b)t \Longrightarrow M_E=\begin{pmatrix}4 & 1\\ 0 & -1\end{pmatrix}.$
\\Eigen-stuff of $M_E$: Characteristic polynomial: $\det(M_E-\lambda I)=(4-\lambda)(-1-\lambda)$
$\Longrightarrow$ eigenvalues $\lambda_1=4,;\lambda_2=-1$.
\\Eigenvector for $\lambda_1=4$: $(M_E-4I)v=0\Longrightarrow v_1=\begin{pmatrix}1\\0\end{pmatrix}.$
\\Eigenvector for $\lambda_2=-1$: $(M_E+I)v=0\Longrightarrow v_2=\begin{pmatrix}1\\-5\end{pmatrix}.$
\\So $B_1={v_1,v_2}$ with $v_1=(1,0)^{\top},;v_2=(1,-5)^{\top}$
\\A diagonal basis $C$ for $P_1$ and the diagonal matrix $D$: Identify $(a,b)^{!\top}\leftrightarrow a+bt$:
$v_1\leftrightarrow 1,\qquad v_2\leftrightarrow 1-5t,$
so $C={,1,;1-5t,}$. Because $T(1)=4$ and $T(1-5t)=-(1-5t)$, $D_{C}=\begin{pmatrix}4 & 0\\ 0 & -1\end{pmatrix}.$
\\Change-of-coordinates matrices: Let $F={e_1,e_2}$ with $e_1=(1,0)^{\top},,e_2=(0,1)^{\top}$. Matrix whose columns are $v_1,v_2$: $P_{F\leftarrow B_1}=\begin{pmatrix}1 & 1\\ 0 & -5\end{pmatrix}.$
Its inverse gives coords in $B_1$:
$P_{B_1\leftarrow F}=P_{F\leftarrow B_1}^{-1}=\begin{pmatrix}1 & \tfrac15\\ 0 & -\tfrac15\end{pmatrix}.$
\\Coordinates in $C$ and the diagonal action: Write $f(t)=5+3t$ in $C$:
$\alpha,1+\beta,(1-5t)=5+3t \Longrightarrow \beta=-\tfrac35,;\alpha=\tfrac{28}{5}.$
Hence $[f]_C=\begin{pmatrix}\tfrac{28}{5}\\-\tfrac35\end{pmatrix}$
\\Compute $T(f(t))$ and express in $C$:
\\$T(f)=T(5+3t)=23-3t,\qquad \gamma,1+\delta,(1-5t)=23-3t \Longrightarrow \delta=\tfrac35,;\gamma=\tfrac{112}{5},$
\\so $[T(f)]_C=\begin{pmatrix}\tfrac{112}{5}\\\tfrac35\end{pmatrix}$.
\\Verify diagonal action:
$D,[f]_C=\begin{pmatrix}4 & 0\ 0 & -1\end{pmatrix} \begin{pmatrix}\tfrac{28}{5}\\-\tfrac35\end{pmatrix}=\begin{pmatrix}\tfrac{112}{5}\\[4pt]\tfrac35\end{pmatrix} =[T(f)]_C.$
\textbf{To find the eigenspace corresponding to an eigenvalue } $\lambda$ \textbf{ of a matrix } $A$:

\begin{enumerate}
  \item Compute the eigenvalue $\lambda$ from the characteristic equation: $\det(A - \lambda I) = 0$
  \item Form the matrix $A - \lambda I$ and solve the homogeneous system:
  $(A - \lambda I)\vec{x} = \vec{0}$
  \item The solution set (nullspace of $A - \lambda I$) is the eigenspace corresponding to $\lambda$.
  \item A - 4I = $\begin{bmatrix} 0 & 1 \\ 0 & -2 \end{bmatrix}\quad \Rightarrow \quad \begin{bmatrix} 0 & 1 \\ 0 & -2 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}= \begin{bmatrix} 0 \\ 0 \end{bmatrix}$
  \item Solve the system: $y = 0 \Rightarrow x \text{ free} \Rightarrow \vec{x} = x\begin{bmatrix} 1 \\ 0 \end{bmatrix}$
  \item $A - 2I = \begin{bmatrix} 2 & 1 \\ 0 & 0 \end{bmatrix}\quad \Rightarrow \quad\begin{bmatrix} 2 & 1 \\ 0 & 0 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix}= \begin{bmatrix} 0 \\ 0 \end{bmatrix}\Rightarrow 2x + y = 0$
  \item $y = -2x \Rightarrow \vec{x} = x\begin{bmatrix} 1 \\ -2 \end{bmatrix}$
\end{enumerate}
\textbf{Step-by-step: How to find an eigenvector}
\begin{enumerate}
    \item Find the eigenvalues by solving: $\det(A - \lambda I) = 0$
    \item For each eigenvalue $\lambda$, solve the equation: $(A - \lambda I)\vec{x} = \vec{0}$
    \item The nonzero solutions $\vec{x}$ are the eigenvectors corresponding to $\lambda$
\end{enumerate}
\textbf{To diagonalize a matrix } $A$:
\begin{enumerate}
    \item Find the eigenvalues $\lambda_1, \lambda_2, \dots$ by solving: $\det(A - \lambda I) = 0$
    \item For each eigenvalue $\lambda_i$, find a basis for its eigenspace by solving: $(A - \lambda_i I)\vec{x} = \vec{0}$
    \item If you find $n$ linearly independent eigenvectors (for an $n \times n$ matrix), form matrix $P$ with these vectors as columns.
    \item Form diagonal matrix $D$ with the eigenvalues on the diagonal in the same order as their corresponding eigenvectors in $P$.
    \item $A = P D P^{-1}$
\end{enumerate}
\columnbreak \pagebreak \textbf{Goal:} Given a basis $\{ \vec{v}_1, \vec{v}_2, \dots, \vec{v}_n \}$ for a subspace of $\mathbb{R}^n$, construct an \emph{orthonormal} basis $\{ \vec{u}_1, \vec{u}_2, \dots, \vec{u}_n \}$.

\textbf{Steps:}
\begin{align*}
\vec{u}_1 &= \frac{\vec{v}_1}{\|\vec{v}_1\|} \\
\vec{u}_2' &= \vec{v}_2 - \text{proj}_{\vec{u}_1} \vec{v}_2 
= \vec{v}_2 - \left( \frac{\vec{v}_2 \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1} \right)\vec{u}_1 \\
\vec{u}_2 &= \frac{\vec{u}_2'}{\|\vec{u}_2'\|} \\
\vec{u}_3' &= \vec{v}_3 - \text{proj}_{\vec{u}_1} \vec{v}_3 - \text{proj}_{\vec{u}_2} \vec{v}_3 \\
\vec{u}_3 &= \frac{\vec{u}_3'}{\|\vec{u}_3'\|} 
\end{align*}

\textbf{Example: Orthonormalize } $\vec{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \ 
\vec{v}_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$

\begin{itemize}
  \item Step 1:
  \[
  \vec{u}_1 = \frac{\vec{v}_1}{\|\vec{v}_1\|} = 
  \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}
  \]

  \item Step 2:
  \[
  \text{proj}_{\vec{u}_1} \vec{v}_2 = 
  \left( \frac{\vec{v}_2 \cdot \vec{u}_1}{\vec{u}_1 \cdot \vec{u}_1} \right)\vec{u}_1 
  = 0 \Rightarrow \vec{u}_2' = \vec{v}_2
  \]

  \[
  \vec{u}_2 = \frac{\vec{v}_2}{\|\vec{v}_2\|} = 
  \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}
  \]

  \item So the orthonormal basis is:
  \[
  \left\{
    \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix},\ 
    \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}
  \right\}
  \]
\end{itemize}


\Large Proofs
\footnotesize
% ----------------------------------------
% Solutions
% ----------------------------------------

\paragraph{Test 2 Problem 7 (solution).}
Let the columns of $A$ be $a_1,\dots ,a_n$ and the columns of $B$ be $b_1,\dots ,b_p$.  
Matrix multiplication gives  
$$
AB \;=\; A[b_1\, b_2\, \dots\, b_p] \;=\; [\,Ab_1\, Ab_2\, \dots\, Ab_p\,].
$$
Each vector $Ab_j$ is a linear combination of the columns of $A$ because $Ab_j = a_1b_{1j} + \dots + a_nb_{nj}$.  
Hence every column of $AB$ lies in the column space of $A$, so  
$$
\text{Col}(AB) \subseteq \text{Col}(A) \quad\Longrightarrow\quad
\operatorname{rank}(AB) \le \operatorname{rank}(A).
$$

\vspace{1em}

\paragraph{Test 2 Problem 8 (solutions, 4 points each).}

\begin{enumerate}[label=(\alph*)]

\item \textbf{False.}  True or False: Row operations can change the row space of a matrix.
Elementary row operations do \emph{not} change the row space.  
Each new row is a linear combination of the original rows, and each original row is a linear combination of the new rows, so the span of the rows is preserved.  
Therefore row operations leave the row space unchanged.

\item \textbf{True.}  
True or False: A change of coordinate matrix is always square. A change–of–coordinate matrix converts coordinate column vectors from one basis of a vector space $V$ to another basis of $V$.  
Both bases have $\dim V$ vectors, so the matrix has the same number of rows and columns and is therefore square.

\item \textbf{False.}  True or False: If $B$ is obtained by scaling one row of $A$, then $\det(B)=\det(A)$.
Scaling a single row by a non-zero scalar $k$ multiplies the determinant by $k$.  
Example:  
$$
A=\begin{bmatrix}1&2\\3&4\end{bmatrix},\quad
B=\begin{bmatrix}3&6\\3&4\end{bmatrix}\ (\text{first row multiplied by }3).
$$
$\det(A) = -2$ while $\det(B) = 3(-2) = -6 \ne \det(A)$.

\item \textbf{True.}  True or False: The dimension of the row and column space of $A$ is the same even if $A$ is not a square matrix.
The row rank of a matrix equals its column rank, a fundamental theorem of linear algebra.  
Thus $\dim\text{Row}(A)=\dim\text{Col}(A)$ even when $A$ is rectangular.
\end{enumerate}
% ----------------------------------------
% Problem 12 and solutions
% ----------------------------------------

\paragraph{Test 2 Problem 12.}
Let $V = M_{2\times 2}$ be the vector space over $\mathbb{R}$ of all real $2\times 2$ matrices.  
Let $W$ be the subset of $V$ consisting of all symmetric matrices, i.e.
$$
W \;=\;\bigl\{\begin{bmatrix} a & b \\ b & c \end{bmatrix}\,\bigm|\,a,b,c\in\mathbb{R}\bigr\}.
$$

\begin{enumerate}[label=(\alph*)]

\item \textbf{Prove that $W$ is a subspace of $V$.}

\textit{Solution.}  
To show that $W$ is a subspace we verify the three axioms:

\begin{itemize}
\item \emph{Zero element.}  
$\begin{bmatrix}0&0\\0&0\end{bmatrix}$ is symmetric, so $0_V\in W$.

\item \emph{Closed under addition.}  
Let $A=\begin{bmatrix}a&b\\b&c\end{bmatrix}$ and $B=\begin{bmatrix}d&e\\e&f\end{bmatrix}$ in $W$.  
Then
$$
A+B = \begin{bmatrix} a+d & b+e \\ b+e & c+f \end{bmatrix},
$$
which is symmetric because the off-diagonal entries match. Hence $A+B\in W$.

\item \emph{Closed under scalar multiplication.}  
For any $\lambda\in\mathbb{R}$ and $A=\begin{bmatrix}a&b\\b&c\end{bmatrix}\in W$,
$$
\lambda A = \begin{bmatrix} \lambda a & \lambda b \\ \lambda b & \lambda c \end{bmatrix},
$$
which is again symmetric. Thus $\lambda A\in W$.
\end{itemize}
Since all three conditions hold, $W$ is a subspace of $V$.

\item \textbf{Find a basis of $W$ and determine $\dim W$.}

\textit{Solution.}  
Every $A\in W$ can be written uniquely as
$$
A = a\begin{bmatrix}1&0\\0&0\end{bmatrix}
  + b\begin{bmatrix}0&1\\1&0\end{bmatrix}
  + c\begin{bmatrix}0&0\\0&1\end{bmatrix}.
$$
Hence the three symmetric matrices
$$
E_1=\begin{bmatrix}1&0\\0&0\end{bmatrix},\quad
E_2=\begin{bmatrix}0&1\\1&0\end{bmatrix},\quad
E_3=\begin{bmatrix}0&0\\0&1\end{bmatrix}
$$
span $W$. They are linearly independent because a linear combination $xE_1+yE_2+zE_3=0$ forces $x=y=z=0$.  
Thus $\{E_1,E_2,E_3\}$ is a basis of $W$ and
$$
\dim W = 3.
$$

\end{enumerate}
\textbf{Test 3 Problem 1:}
\begin{enumerate}[label=(\alph*)]

    \item If $A$ and $B$ have the same eigenvalues then $A$ is similar to $B$.\\[2pt]
    \textbf{False.}\;  
    Take
    $
    A = I_2
    =
    \begin{bmatrix}
    1 & 0\\
    0 & 1
    \end{bmatrix},
    \quad
    B =
    \begin{bmatrix}
    1 & 1\\
    0 & 1
    \end{bmatrix}
    $.
    Both have the single eigenvalue $1$, yet $A$ is diagonal while $B$ is a non-diagonal Jordan block, so no invertible matrix $P$ satisfies $A = PBP^{-1}$. Hence they are not similar.
    
    %--------------------------------------------------
    \item If $v$ is an eigenvector of $A$ with eigenvalue $2$, then $4$ is an eigenvalue of the matrix $2A$.\\[2pt]
    \textbf{True.}\;  
    Given $Av = 2v$ with $v \ne 0$, compute
    $
    (2A)v = 2(Av) = 2\cdot 2v = 4v
    $.
    Thus $v$ is an eigenvector of $2A$ and its eigenvalue is $4$.
    
    %--------------------------------------------------
    \item The eigenvalues of $A$ are the same as the eigenvalues of $A^{T}$.\\[2pt]
    \textbf{True.}\;  
    The characteristic polynomials satisfy
    $
    \det(\lambda I - A^{T}) = \det\!\bigl((\lambda I - A)^{T}\bigr)
            = \det(\lambda I - A)
    $
    because the determinant of a matrix equals the determinant of its transpose.  
    Therefore $\lambda$ is a root of one polynomial exactly when it is a root of the other, so $A$ and $A^{T}$ share the same eigenvalues (including algebraic multiplicities).
    
    %--------------------------------------------------
    \item If $v_{1}$ and $v_{2}$ are eigenvectors of $A$ with eigenvalue $\lambda$, then $v_{1}+v_{2}$ is an eigenvector of $A$ with eigenvalue $\lambda$.\\[2pt]
    \textbf{False.}\;  
    Although the eigenspace $E_{\lambda}$ is a subspace, the sum $v_{1}+v_{2}$ could be the zero vector, which is \emph{not} an eigenvector.  
    Example: let $A = I_2$, $\lambda = 1$, $v_{1} = \begin{bmatrix}1\\0\end{bmatrix}$, $v_{2} = \begin{bmatrix}-1\\0\end{bmatrix}$.  
    Both $v_{1}$ and $v_{2}$ are eigenvectors, but $v_{1}+v_{2}=0$.  
    Since zero cannot serve as an eigenvector, the statement fails.
    If one adds the non-zero requirement, then the claim becomes true because $A(v_{1}+v_{2}) = \lambda(v_{1}+v_{2})$.
    \end{enumerate}
\textbf{EVERYTHING ONWARDS FROM HERE IS FROM PRACTICE TESTS}
% ------------------------------------------------------------------
% Problems and solutions
% ------------------------------------------------------------------

% ---------------------------- Problem 7 ---------------------------
\paragraph{Problem 7.}
For each statement decide whether it is \textbf{TRUE} or \textbf{FALSE}.  
If it is true give a proof.  
If it is false produce a counterexample and name the theorem that shows why the conclusion fails.

\begin{enumerate}[label=(\alph*)]
\setcounter{enumi}{5}

\item If $A$ and $B$ are both invertible then $(A^{-1}B)^{T}$ is also invertible.\\
\textbf{True.}  
$A^{-1}$ and $B$ are invertible so $A^{-1}B$ is invertible by the Product Inverse Theorem  
$\bigl((A^{-1}B)^{-1}=B^{-1}A\bigr)$.  
The transpose of any invertible matrix is invertible with inverse $\bigl((A^{-1}B)^{T}\bigr)^{-1}=(A^{-1}B)^{-T}$.  
Hence $(A^{-1}B)^{T}$ is invertible.

\item If $A^{4}=3I$ then $A$ is invertible.\\
\textbf{True.}  
Multiply $A^{4}=3I$ on the left by $\tfrac{1}{3}A^{3}$ to obtain  
$\tfrac{1}{3}A^{3}A^{4}=\tfrac{1}{3}A^{3}\,3I \Longrightarrow A=\tfrac{1}{3}A^{3}$.  
Therefore $A^{-1}=\tfrac{1}{3}A^{3}$ and $A$ is invertible.  
(The theorem used is: if a matrix admits a two–sided inverse it is invertible.)

\item If $AB=B$ for some matrix $B\neq 0$ then $A$ is invertible.\\
\textbf{False.}  
Let 
$
A=\begin{bmatrix}1&0\\0&0\end{bmatrix},\,
B=\begin{bmatrix}1&0\\0&0\end{bmatrix}.
$  
Then $AB=B$ with $B\neq 0$ yet $A$ is not invertible (its second row is zero so its determinant is zero).  
Theorem used: a square matrix is invertible exactly when its determinant is non–zero.

\item If $E$ is an elementary matrix then $\det(E)=1$.\\
\textbf{False.}  
Row–swap elementary matrices satisfy $\det(E)=-1$ and row–scaling matrices satisfy $\det(E)=k$ where $k$ is the scaling factor.  
Example  
$
E=\begin{bmatrix}0&1\\1&0\end{bmatrix}
$  
is a row–swap matrix with determinant $-1$.  
Theorem used: a determinant changes sign under a row swap and scales by $k$ when a row is multiplied by $k$.

\item $(AB)^{2}=A^{2}B^{2}$.\\
\textbf{False.}  
Choose  
$
A=\begin{bmatrix}0&1\\0&0\end{bmatrix},\,
B=\begin{bmatrix}0&0\\1&0\end{bmatrix}.
$  
Then $AB=\begin{bmatrix}1&0\\0&0\end{bmatrix}$ so $(AB)^{2}=AB$, whereas  
$A^{2}=0$ so $A^{2}B^{2}=0$.  
They are different.  
Theorem used: in general matrix multiplication is not commutative so powers do not distribute in that way.

\item If $A$ is a square matrix with two identical rows then $A$ cannot be invertible.\\
\textbf{True.}  
Identical rows are linearly dependent so the rows of $A$ fail to form a basis of $\mathbb R^{n}$.  
Hence $\det(A)=0$ and $A$ is not invertible.  
(Theorem used: a matrix is invertible exactly when its determinant is non–zero.)

\end{enumerate}

% ---------------------------- Problem 17 ---------------------------
\paragraph{Problem 17.}
Calculate the area of the parallelogram determined by the vertices  
$(-1,-2),\ (1,4),\ (6,2),\ (8,8)$.

\textit{Solution.}  
Take adjacent edges based at $(-1,-2)$  
\[
v_{1}=(1,4)-(-1,-2)=(2,6),\quad
v_{2}=(6,2)-(-1,-2)=(7,4).
\]
The area is the absolute value of the determinant formed by $v_{1}$ and $v_{2}$  
\[
\text{Area}=\lvert 2\cdot 4-6\cdot 7\rvert=\lvert 8-42\rvert=34.
\]

% ---------------------------- Problem 18 ---------------------------
\paragraph{Problem 18.}
Assume  
$\det\!\begin{bmatrix}a&b\\c&d\end{bmatrix}=2$.  
Evaluate
\[
\det\!\begin{bmatrix}
2 & -2 & 0\\
c+1 & -1 & 2a\\
d-2 & 2 & 2b
\end{bmatrix}.
\]

\textit{Solution.}  
A direct expansion gives  
\[
\det = 4(-ad+bc)= -4(ad-bc)= -4\cdot 2 = -8.
\]

% ---------------------------- Problem 26 ---------------------------
\paragraph{Problem 26.}
Let $B$ be a basis of a vector space $V$.

\begin{enumerate}[label=(\alph*)]
\item If one vector is removed from $B$ then the remaining set does not span $V$.\\[2pt]
\textit{Proof.}  
$B$ has $\dim V$ vectors.  
Removing one vector leaves $\dim V -1$ vectors.  
No set with fewer than $\dim V$ vectors can span $V$.

\item If one additional vector from $V$ is added to $B$ then the enlarged set is not linearly independent.\\[2pt]
\textit{Proof.}  
Any set in $V$ with more than $\dim V$ vectors is linearly dependent.  
Adding one vector to the $\dim V$ vectors of $B$ produces $\dim V+1$ vectors, hence dependence follows.
\end{enumerate}

% ---------------------------- Problem 27 ---------------------------
\paragraph{Problem 27.}
Let $T:V\to W$ be a linear transformation.

\begin{enumerate}[label=(\alph*)]
\item The range of $T$ is a subspace of $W$.\\[2pt]
\textit{Proof.}  
The zero vector of $W$ equals $T(0)$.  
If $y_{1}=T(x_{1})$ and $y_{2}=T(x_{2})$ then for any scalars $\alpha,\beta$  
$\alpha y_{1}+\beta y_{2}=T(\alpha x_{1}+\beta x_{2})$ which is in the range.  
Hence the range satisfies the subspace test.

\item The kernel of $T$ is a subspace of $V$.\\[2pt]
\textit{Proof.}  
$T(0)=0$ so $0$ lies in the kernel.  
If $x_{1},x_{2}\in\ker T$ then $T(x_{1})=T(x_{2})=0$.  
For any scalars $\alpha,\beta$ we have  
$T(\alpha x_{1}+\beta x_{2})=\alpha T(x_{1})+\beta T(x_{2})=0$,  
so $\alpha x_{1}+\beta x_{2}\in\ker T$.  
Therefore the kernel is a subspace.
\end{enumerate}
% -------------------------------------------------------------
% Additional Linear-Algebra Problems — Statements and Solutions
% -------------------------------------------------------------

% --------------------------- Problem 28 -----------------------
\paragraph{28) Let $A$ be a $4\times 7$ matrix.}

\begin{enumerate}[label=(\alph*)]
\item \textbf{Find all possible values of $\operatorname{nullity}(A)$.}

By the Rank–Nullity Theorem,
$
\operatorname{rank}(A)+\operatorname{nullity}(A)=7.
$
Because $A$ has $4$ rows, $\operatorname{rank}(A)\le 4$.  
Hence
$
\operatorname{nullity}(A)=7-\operatorname{rank}(A)\in\{7,6,5,4,3\}.
$

\item \textbf{If $T(\vec x)=A\vec x$, could $T$ be one-to-one?}

No. A linear map is one-to-one exactly when its nullity is $0$, but in part (a) we saw $\operatorname{nullity}(A)\ge 3$.

\item \textbf{If $T(\vec x)=A\vec x$, what is the codomain of $T$? Could $T$ be onto its codomain?}

The codomain is $\mathbb R^{4}$ (one coordinate for each row).  
$T$ is onto iff $\operatorname{rank}(A)=4$. This value is attainable, so $T$ \emph{could} be onto.

\end{enumerate}

% --------------------------- Problem 5 ------------------------
\paragraph{5) Closure of the null space.}

Let $A\vec u=\vec 0$ and $A\vec v=\vec 0$ and let $a,b\in\mathbb R$.  
Then
$
A(a\vec u+b\vec v)=aA\vec u+bA\vec v=a\vec 0+b\vec 0=\vec 0.
$
Therefore $\vec w=a\vec u+b\vec v$ lies in $\operatorname{Nul}A$.

% --------------------------- Problem 6 ------------------------
\paragraph{6) Linear dependence preserved under a linear map.}

Suppose $\{\,\vec v_{1},\dots,\vec v_{n}\,\}\subset V$ is linearly dependent, so
$
c_{1}\vec v_{1}+\dots+c_{n}\vec v_{n}=\vec 0
$
for some scalars not all zero.  
Apply a linear transformation $T:V\to W$:
$
c_{1}T(\vec v_{1})+\dots+c_{n}T(\vec v_{n})=T(\vec 0)=\vec 0.
$
Because not all $c_{i}$ are zero, $\{\,T(\vec v_{1}),\dots,T(\vec v_{n})\,\}$ is linearly dependent in $W$.

% --------------------------- Problem 7 ------------------------
\paragraph{7) One-to-one maps carry independence to independence.}

Let $T:V\to W$ be one-to-one and assume $\{\,\vec v_{1},\dots,\vec v_{n}\,\}$ is linearly \emph{independent}.  
Suppose a linear relation
$
c_{1}T(\vec v_{1})+\dots+c_{n}T(\vec v_{n})=\vec 0
$
holds. Apply $T^{-1}$ on its image (possible because $T$ is injective):
$
T(c_{1}\vec v_{1}+\dots+c_{n}\vec v_{n})=\vec 0 \;\Longrightarrow\;
c_{1}\vec v_{1}+\dots+c_{n}\vec v_{n}=\vec 0.
$
Independence of the $\vec v_{i}$ forces all $c_{i}=0$, so the images are also independent.

% --------------------------- Problem 4 ------------------------
\paragraph{4) Existence of solutions for every right-hand side.}

A homogeneous system of $40$ equations in $42$ unknowns can have at most 
$
\operatorname{nullity}=42-40=2
$
(independent free variables).  
The statement that there are $5$ linearly independent solutions contradicts this bound, so the described situation cannot occur.  
Even ignoring that contradiction, a system is consistent for every right-hand side $\vec b$ only when the coefficient matrix has full row rank (its rows span $\mathbb R^{40}$).  
Here $\operatorname{rank}\le 40$ but need not be $40$, so the system need not have a solution for every $\vec b$.
% -------------------------------------------------------------
% Problem 11
% For each statement, decide whether it is \textbf{TRUE} or \textbf{FALSE}.
% Prove a true statement or give a counterexample if it is false.
% -------------------------------------------------------------

\begin{enumerate}[label=(\alph*)]

    \item If $A$ is an $m\times n$ matrix and the linear transformation $\vec x\mapsto A\vec x$ is \emph{onto}, then $\operatorname{rank}A=m$.\\
    \textbf{True.}  Onto means the column space equals $\mathbb R^{m}$, whose dimension is $m$, so the rank is $m$.
    
    \item If $A$ is invertible, then $A^{2}\ne 0$.\\
    \textbf{True.}  If $A^{2}=0$, multiply on the left by $A^{-1}$ to get $A=0$, contradicting invertibility.
    
    \item If $S$ is a linearly independent set, then $S$ is a basis for $V$.\\
    \textbf{False.}  In $\mathbb R^{3}$ the set $\{(1,0,0)\}$ is independent but not a basis because it does not span $\mathbb R^{3}$.
    
    \item If $A$ and $B$ are square and $AB=BA$, then $(AB)^{2}=A^{2}B^{2}$.\\
    \textbf{True.}
    \[
    (AB)^{2}=ABAB=A(BA)B=A(AB)B=A^{2}B^{2}.
    \]
    
    \item If $A$ is similar to $B$, then $A^{2}$ is similar to $B^{2}$.\\
    \textbf{True.}  $B=P^{-1}AP$ implies $B^{2}=P^{-1}A^{2}P$.
    
    \item If $A$ and $B$ are both invertible, then $(A^{-1}B)^{T}$ is invertible.\\
    \textbf{True.}  $A^{-1}B$ is invertible and the transpose of an invertible matrix is invertible.
    
    \item $A$ is diagonalizable if and only if $A$ has $n$ distinct eigenvalues.\\
    \textbf{False.}  The identity matrix $I_{n}$ is diagonalizable but has only one eigenvalue.
    
    \item If $AP=PD$ with $D$ diagonal, then the non-zero columns of $P$ are eigenvectors of $A$.\\
    \textbf{True.}  Writing $P=[\vec p_{1}\ \dots\ \vec p_{k}]$ and $D=\operatorname{diag}(\lambda_{1},\dots,\lambda_{k})$ gives $A\vec p_{i}=\lambda_{i}\vec p_{i}$.
    
    \item If $A$ is invertible and $1$ is an eigenvalue of $A$, then $1$ is an eigenvalue of $A^{-1}$.\\
    \textbf{True.}  Eigenvalues of $A^{-1}$ are reciprocals of those of $A$; $1^{-1}=1$.
    
    \item If $A$ is row-equivalent to the identity matrix, then $A$ is diagonalizable.\\
    \textbf{False.}  $A=\begin{bmatrix}1&1\\0&1\end{bmatrix}$ row-reduces to $I_{2}$ but is not diagonalizable (one Jordan block).
    
    \item If $\vec u,\vec v,\vec w$ lie in a vector space of dimension 2, then $\vec w$ is a linear combination of $\vec u$ and $\vec v$.\\
    \textbf{False.}  Take $\vec u=\vec v=(1,0)$ in $\mathbb R^{2}$; $\vec w=(0,1)$ is not a combination of $\vec u$ and $\vec v$.
    
    \item If $A^{2}=2I$, then $A$ is invertible.\\
    \textbf{True.}  Multiply $A^{2}=2I$ on the left by $\tfrac12A$ to obtain $A=\tfrac12A^{3}$, so $A^{-1}=\tfrac12A$.
    \end{enumerate}
    % Effects of elementary row operations on the determinant

\begin{tabular}{|l|l|l|}
    \hline
    \textbf{Row operation} & \textbf{New determinant} & \textbf{Reason} \\ \hline
    Swap rows $i$ and $j$        & $\det(A') = -\,\det(A)$ & Alternating property \\ \hline
    Multiply row $i$ by $k\neq 0$ & $\det(A') = k\,\det(A)$ & Multilinearity \\ \hline
    $R_j \gets R_j + k\,R_i$ ($i\neq j$) & $\det(A') = \det(A)$ & Multilinearity and zero for equal rows \\ \hline
    \end{tabular}
    
\end{multicols*}


\end{document}